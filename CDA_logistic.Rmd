---
title: "Ciência de Dados - Regressão Logística."
author: "Rogério de Oliveira"
date: "`r Sys.Date()`"
description: Ciência de Dados - Regressão Logística.
---

# Regressão Logística
***
<img src="http://meusite.mackenzie.br/rogerio/mackenzie_logo/UPM.2_horizontal_vermelho.jpg"  width=300, align="right">
<br>
<br>
<br>
<br>
<br>


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')

```

Aqui você empregará vários dos conceitos aprendidos antes para entender e aplicar um modelo de regressão logística para classificação de dados. Esse é um importante modelo de classificação e desempenha um papel fundamental para o entendimento de modelos mais gerais de aprendizado de máquina supervisionados.

## Introdução 

A regressão logística modela as probabilidades para problemas de classificação binários (com dois resultados possíveis, como 'y/n' ou '0/1') e pode ser entendido como uma extensão dos modelos de regressão linear para problemas de classificação.

O modelo de regressão linear pode funcionar bem para regressão, mas falha na classificação. No caso de duas classes, você poderia rotular uma das classes com 0 e a outra com 1 e usar a regressão linear. Tecnicamente isso funciona e certamente obterá os coeficientes da regressão linear, afinal você já aprendeu que para quaisquer conjuntos de pontos $(X, y)$ podemos calcular os coeficientes de um modelo linear. Mas para classificação essa abordagem apresenta muitos problemas. Como seu resultado não é uma probabilidade, mas a interpolação linear dos pontos, não há um limite significativo no qual você possa distinguir uma classe da outra. 

Um modelo simples pode mostrar a ineficiência da regressão linear para a classificação. Veja a abaixo o uso de regressão linear para a classificação dos pontos verdes e vermelhos abaixo.


```{r}
# adaptado de https://christophm.github.io/interpretable-ml-book/logistic.html
# e https://stats.stackexchange.com/questions/22381/why-not-approach-classification-through-regression

df = data.frame(x = c(1,2,3,8,9,10,11,9),
  y = c(0,0,0,1,1,1,1, 0))

df_extra  = data.frame(x=c(df$x, 7, 7, 7, 20, 19, 5, 5, 4, 4.5),
  y=c(df$y, 1,1,1,1, 1, 1, 1, 1, 1))

# par(mfrow = c(1, 3))
layout.matrix <- matrix(c(1, 1, 1, 0), nrow = 2, ncol = 2)

layout(mat = layout.matrix,
       heights = c(2, 2), # Heights of the two rows
       widths = c(2, 2)) # Widths of the two columns

plot(df$x, df$y, col=df$y+2, pch=17)
fit = lm(y ~ x,data=df)
abline(coefficients(fit),col='red',lty=2)
points(df$x, df$y, col=df$y+2, pch=17, cex=1.5)

plot(df_extra$x, df_extra$y, col=df_extra$y+2)
fit = lm(y ~ x,data=df)
abline(coefficients(fit),col='red',lty=2)
points(df_extra$x, df_extra$y, col=df_extra$y+2, pch=17, cex=1.5)

plot(df$x, df$y, col=df$y+2, pch=17)
fit = lm(y ~ x,data=df)
abline(coefficients(fit),col='red',lty=2)
polygon(c(0,12,12),c(predict(fit,data.frame(x = c(0,12))),-0.1), col='#FF9966')
polygon(c(0,12,0), c(predict(fit,data.frame(x = c(0,12))),1.1), col='#CCFF00')
points(df$x, df$y, col=df$y+2, pch=17, cex=1.5)

plot(df_extra$x, df_extra$y, col=df_extra$y+2)
fit = lm(y ~ x,data=df)
abline(coefficients(fit),col='red',lty=2)
polygon(c(0,21,21),c(predict(fit,data.frame(x = c(0,21))),-0.1), col='#FF9966')
polygon(c(0,21,0), c(predict(fit,data.frame(x = c(0,21))),1.1), col='#CCFF00')
points(df_extra$x, df_extra$y, col=df_extra$y+2, pch=17, cex=1.5)

```

O primeiro conjunto de pontos é linearmente separável e a regressão linear permite classificar os dados com apenas um erro de classificação (o ponto vermelho de menor valor $x$). Mas ao inserirmos novos dos o modelo falha. 


## Modelo logístico

Para resolver esse problema, em vez de modelarmos diretamente a probabilidade de uma classe modelaremos por uma regressão linear o que é conhecido como log de probabilidades:

$$ log( \frac{p}{1-p}) = a_0 + a_1 x_1 + ... + a_n x_n $$

Os resultados da regressão logística vão então informar sobre as chances ('Odds') e razão das chances ('Odds Ratio') e não exatamente probabilidades (por exemplo, em alguns casos os valores podem não ter soma 1), mas podemos entender que, uma *change* reflete em uma probabilidade maior daquele evento ocorrer. Se um evento tem probabilidade p, suas chances são p / (1-p), e é por isso que a esquerda lado é chamado de "odds logarítmicas" ou "logit", e podemos obter a 
probabilidade de chances invertendo a função acima:

$$ p = 1 / (1 + e^{- ( a_0 + a_1 x_1 + ... + a_n x_n ) })$$

Essa função é conhecida como  função logística ou sigmóide, $\sigma$, é definida como:


$$\sigma(x)=\frac{1}{1+e^{-x}}$$
Essa função tem o seguinte gráfico:


```{r}
library(latex2exp)

logistic = function(x)
{1 / (1 + exp(-x))}

x = seq(-5,5,0.01)
plot(x, logistic(x), type='l', col='red')

title('Logistic Function')
text(-2,0.7, TeX('$\\sigma(x) = \\frac{1}{1+e^{-x}}$') )

```


Note que os valores dessa função variam de 0 a 1 e é portanto, possível empregar essa função como uma medida de probabilidade das chances. 

Esse procedimento funciona muito melhor para classificação e podemos usar 0.5 como o valor limite de probabilidades entre as classes, isto é, valores $p < 0.5$ para uma classe e valores $p \ge 0.5$ para outra. 

## Estimando os parâmetros

Como estimar então os coeficientes $a_0, a_1, ...$. De fato, esse cálculo já não é tão simples como o da regressão linear. Do mesmo modo que na regressão linear queremos minimizar o erro (o que é o mesmo que, maximizar os *acertos*), mas no lugar de empregarmos uma solução algébrica (como o sistema de derivadas igualado a 0 que vimos anteriormente), aqui isso é feito em geral de forma numérica, em um procedimento iterativo de aproximações sucessivas e algo que tem uma forte aplicação em muitos outros modelos de aprendizado de máquina. Como você poderá ver em outros cursos, em geral o aprendizado dos parâmetros pode ser resumido a maximizar ou minimizar uma determinada função objetivo. O procedimento, para classificação entre duas classes possíveis '0' e '1' (poderiam ser 'y' e 'n', ou ainda 'red' e 'green') é de forma geral o seguinte:

1. Para cada amostra que realmente pertença a classe '1', gostaríamos que p ficasse perto de 1, e para cada amostra que não fosse '1', gostaríamos que p fosse próximo de 0 (e, portanto, 1-p deve ser próximo a 1). 

2. Assim, tomamos o produto de p sobre todas as amostras '1' com o produto de 1-p sobre todas as não '1' amostras para avaliar a precisão de nossas estimativas para os parâmetros $a_0, a_1, ...$ 

3. Gostaríamos de fazer que a função de probabilidade seja a maior possível (ou seja, o mais próximo possível de 1). Começamos então com um palpite para os parâmetros (sim, um *chute* inicial para o valores!), e vamos ajustando esses valores iterativamente para melhorar a probabilidade até descobrirmos que não podemos mais aumentar essa probabilidade perturbando os coeficientes. 

Um método comum para essa otimização é a descida de gradiente estocástico, algo parecido, por exemplo o método numérico de Newton-Raphson para solução numérica de zero (pontos de máximo ou mínimo) de funções.

Chamamos o processo acima de **treinamento** e, estimados o parâmetros com base em uma amostra podemos então calcular a probabilidade de um novo conjunto de dados ser da classe 0 ou 1 (**predição**). 

### Exemplo

Por exemplo, você pode pensar em um sistema de detecção de fraude de operações de cartão de crétido com base em dados histórico das ocorrências de fraude, valores da operação e uso do cartão nas últimas 24h (duas variáveis que são conhecidas como grandes preditoras de fraude em transações com cartões). Para isso, depois de estimados os valores $a_0, a_1, ...$ com base nas amostras conhecidas, bastaria se aplicar o novo valor `(Valor,Trans24h)` ao modelo logístico estimado: 

$$ P(\text{Fraude} | \text{Valor, Trans24h}) = 1 / (1 + e^{- ( a_0 + a_1 \text{Valor} + a_2 \text{Trans24h} ) })$$

## Vantagens e Cuidados

Um cuidado a ser tomado no uso da regressão logística é que ela pode *sofrer de separação completa*. Isso ocorre quando uma variável preditora separa perfeitamente as duas classes e, neste caso, o modelo de regressão logística não pode mais ser treinado. Isso ocorre porque o peso dessa variável preditora não convergiria, porque o peso ideal seria infinito. Observar isso é importante, mas você talvez não precisa-se de um modelo como esse se tivesse uma regra simples, uma única variável preditora, que pudesse separar as duas classes, e existem técnicas de penalização dos pesos ou definição de uma distribuição de probabilidade dos pesos para minimizar esse problema.

O modelo de regressão logística não é apenas um modelo de classificação, mas também fornece probabilidades. Esta é uma grande vantagem em relação aos modelos que fornecem apenas a classificação final, pois faz uma grande diferença saber se a probalidade daquela classe é 99% ou 51%. Para usar o nosso exemplo, pense  em uma transação de cartão classificada como fraude, ser uma fraude com 99% de chance parece algo bem mais crítico que uma fraude com 51% de chance! 

A regressão logística ainda é um modelo de classificação binária. Mas ele pode ser estendido para classificação multiclasse, o que denominamos de Regressão Multinomial. O procedimento é simples e consiste em criarmos vários classificadores. Se você tem 3 classes, 'Y', 'N', 'O', pode então ter um classificador logístico 'Y' e 'não Y', outro 'N' e 'não Y', ainda 'O' e 'não O' e, do mesmo modo, buscar o maior valor de p entre esses classificadores. Em geral os pacotes de Software como R fazem uma implementação multimodal da classificação logística e, portanto, fica transparente para você a regressão logística ser um procedimento de classificação binária.  

## Exemplo: `mtcars`

Vamos agora empregar a função logística para classificar os veículos como automáticos e não automáticos (atributo `am`), tendo como base as variáveis preditoras `hp`, e peso, `wt`.   

```{r}
head(mtcars)

```

Nessa base de dados você encontra carros automáticos (1 = TRUE) e não automáticos (0 = FALSE):

```{r}
table(mtcars$am)
```
A função `glm()` (*general linear models*) pode ser então empregada para estimar os parâmetros da regressão logística com base nos dados de potência, `hp`, e peso, `wt` dos veículos. Indique o parâmetro `family=binomial` para empregar a função logística no modelo linear generalizado.

A `formula` (primeiro argumento) da função `glm()` segue o mesmo formato que você empregou na regressão linear anteriormente, `y ~ x1 + x2 + ... ` onde `y` é a variável objetivo e `x1 + x2 + ...` são as variáveis preditoras. 


```{r}
fit = glm(formula = am ~ hp + wt, data=mtcars, family=binomial)
summary(fit)
```

Os coeficientes de `hp` e `wt` apresentam p-values < 0.05 e, portanto, são ambos significativos para a predição de `am`.

Feito do treinamento do modelo, isto é, estimado os valores do coeficientes da regressão logística a partir dos dados, podemos empregar o modelo, por exemplo, para fazer a predição de para um carro leve.

```{r}

predict(fit, data.frame(hp=100, wt=1.8))

```

O veículo, tendo $hp=100, wt=1.8$ tem, então, probabilidade de 95% de ser um veículo de transmissão automática. Para obter a classificação 1 (TRUE) e 0 (FALSE) você ainda pode fazer:

```{r}
predict(fit, data.frame(hp=100, wt=1.8)) > 0.5
```
Embora não haja nenhuma restrição *no modelo* quanto ao tipo dos valores da variável objetivo, podendo ser por exemplo assumir valores categóricos como 'Fraude' e 'Não Fraude', o pacote `glm()` requer que esse atributo seja numérico e devolve a probabilidade da classe 1. O exemplo abaixo pode então ser útil para você classificar dados com uma variável objetivo categórica. Para isso adicionamos o atributo `am_text` em `mtcars`. 

```{r}
mtcars$am_text = ifelse(mtcars$am == 1, 'am', 'not am')
head(mtcars)

```
```{r}
am_text_bin = as.numeric(as.factor(mtcars$am_text)) - 1 # converte para 0,1
am_text_bin
```


```{r}
fit = glm(formula = am_text_bin ~ hp + wt, data=mtcars, family=binomial)
summary(fit)
```
```{r}
predict(fit, data.frame(hp=100, wt=1.8)) > 0.5
```
E convertendo para as classes: 

```{r}
if (predict(fit, data.frame(hp=100, wt=1.8)) > 0.5){
  levels(as.factor(mtcars$am_text))[1] 
} else{
  levels(as.factor(mtcars$am_text))[2] 
}
```

## Conclusão Final

Você percorreu aqui um grande número de tópicos de Probabilidade e Estatística e de programação com R. Aprendeu sobre estatísticas descritivas e técnicas de visualização dos dados, entendeu as distribuições de probabilidades discretas e contínuas e, então os principais teoremas da inferência estatística, como a Lei dos Grandes Números e o Teorema Central do Limite. Dentro da inferência estatística aprendeu como fazer inferência de intervalos, quantidades de amostras, e ainda como validar hipóteses sobre os dados. E terminamos fazendo predições de valores e classes, envolvendo uma série dos conceitos aprendidos ao longo do curso.

Esse é um ferramental básico mas que já fornece para você uma série de instrumentos práticos muito úteis em qualquer campo profissional, e que vem sendo cada dia mais exigido à medida que, com a transformação digital, cresce a importância da tomada de decisão a partir dos dados (*Data Driven*) nas empresas.

O que vem a seguir? Esse curso habilita você a seguir os estudos nas áreas ciência e análise de dados em que, além desse ferramental estatístico, empregam-se outras técnicas de aprendizado de máquina e para o quê você teve uma pequena introdução com o estudo da regressão logísica. Uma série de outras técnicas estatísticas importantes, como análises de séries temporais e aglomerados de dados, não foram estudadas aqui, também têm, nos conceitos que aprendeu aqui os seus primeiros fundamentos. Tenho certeza, então, que esse pode não ter sido o seu primeiro contato com estatística e probabilidades, mas certamente não será o último, e você sempre poderá voltar aqui para retomar esses conceitos! ;-)


## Exercícios

### Exercício **RESOLVIDO**
Empregue o modelo anterior e obtenha a predição para todos os veículos de `mtcars`. Compare os valores da predição com os valores reais da base. Verifique o número de acertos e erros do modelo.

```{r}
newdata = mtcars[,c('hp','wt')]
prediction = predict(fit, newdata, type="response")
cbind(mtcars$am, prediction>0.5)
table(prediction>0.5,mtcars$am)

```

Na diagonal encontram-se os acertos. Fora da diagonal as predições que diferem do valor real.  



### Exercício 
Considere a base.

```{r}
df = read.csv('https://meusite.mackenzie.br/rogerio/TIC/FuelConsumptionCo2.csv',header=T)
head(df)

table(df$FUELTYPE)
```

### Exercício 
Crie um modelo logístico para classificar os veículos, com base nos dados de consumo de combinado de combustível FUELCONSUMPTION_COMB e emissões CO2EMISSIONS, em veículos de gasolina especial 'X' e demais combustíveis (atributo FUELTYPE). 

Dica: FUELTYPE tem mais de dois valores e a regressão logística somente pode classificar em combustível 'X' ou não 'X'. Crie um novo atributo FUELX, com 1 se 'X' ou 0 caso contrário, e faça a regressão logística sobre esse valor.

```{r}
df['FUELX'] = ifelse(df$FUELTYPE=='X',1,0)
head(df[c('FUELTYPE','FUELX')])
table(df$FUELX)

```


```{r}
fit = glm(formula=FUELX ~ FUELCONSUMPTION_COMB + CO2EMISSIONS, data=df)
summary(fit)
```


### Exercício 
Estime a probabilidade de um veículo com 'FUELCONSUMPTION_COMB'=9, 'CO2EMISSIONS'=180 ser um veículo de gasolina especial 'X'.

```{r}
newdata = data.frame('FUELCONSUMPTION_COMB'=9, 'CO2EMISSIONS'=180)
prediction = predict(fit, newdata, type="response")
prediction

```
### Exercício 
Empregue o modelo anterior e obtenha a predição para todos os veículos de df. Compare os valores da predição com os valores reais da base. Verifique o número de acertos e erros do modelo. É um bom modelo?


```{r}
newdata = df[,c('FUELCONSUMPTION_COMB', 'CO2EMISSIONS')]
prediction = predict(fit, newdata, type="response")
matrix = table(prediction>0.5,df$FUELX)
matrix
cat('percentual de acertos: ', sum(diag(matrix))/sum(matrix))

```

```{r}
fit = glm(formula=FUELX ~ CO2EMISSIONS + FUELCONSUMPTION_COMB_MPG, data=df)
summary(fit)

newdata = df[,c('CO2EMISSIONS','FUELCONSUMPTION_COMB_MPG')]
prediction = predict(fit, newdata, type="response")
matrix = table(prediction>0.5, df$FUELX)
matrix
cat('percentual de acertos: ', sum(diag(matrix))/sum(matrix))

```

### Exercício 
Considere a base.

```{r}
library(MASS)
help("biopsy")
bio = na.omit(biopsy[,-c(1)]) # 1 = ID
bio$class = as.numeric(bio$class) - 1 
head(bio)
```

Implemente um modelo logístico para classificação "benign" or "malignant" das instâncias de bio. Compare os valores da sua predição com os valores reais da base.

Dica: para usar todos os atributos de entrada empregue '.' no lugar das variáveis de entrada. 
```{r}
fit = glm(formula=class ~ ., data=bio)
summary(fit)

newdata = bio[,-c(length(bio-1))]
prediction = predict(fit, newdata, type="response")
matrix = table(prediction>0.5,bio$class)
matrix
cat('percentual de acertos: ', sum(diag(matrix))/sum(matrix))

```




## Referências

Navarro, Danielle, **Learning Statistics with R**, disponível em: https://learningstatisticswithr.com/ ( LSR version 0.6 (pdf) ). Acesso: 26/02/2021.
Alternativamente em formato bookdown: https://learningstatisticswithr.com/book/ Acesso: 07/03/2021.

Anunciação, Luis, **Conceitos e análises estatísticas com R e JASP**, disponível em: https://anovabr.github.io/mqt/ Acesso: 10/04/2021.

Ferreira, Eric Batista, Oliveira, Marcelo Silva de. **Introdução à Estatística com R**. Editora Universidade Federal de Alfenas, 2020.

